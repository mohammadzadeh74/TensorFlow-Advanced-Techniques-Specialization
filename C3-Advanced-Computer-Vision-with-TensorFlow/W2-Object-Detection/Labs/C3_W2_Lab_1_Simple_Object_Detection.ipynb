{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmANPR2jhCR6"
   },
   "source": [
    "# Simple Object Detection in Tensorflow\n",
    "\n",
    "This lab will walk you through how to use object detection models available in [Tensorflow Hub](https://www.tensorflow.org/hub). In the following sections, you will:\n",
    "\n",
    "* explore the Tensorflow Hub for object detection models\n",
    "* load the models in your workspace\n",
    "* preprocess an image for inference \n",
    "* run inference on the models and inspect the output\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DkMLuGDhCR6"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OEoRKdmByrb0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import tempfile\n",
    "from six.moves.urllib.request import urlopen\n",
    "from six import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb8MBgTOhCR6"
   },
   "source": [
    "### Download the model from Tensorflow Hub\n",
    "\n",
    "Tensorflow Hub is a repository of trained machine learning models which you can reuse in your own projects. \n",
    "- You can see the domains covered [here](https://tfhub.dev/) and its subcategories. \n",
    "- For this lab, you will want to look at the [image object detection subcategory](https://tfhub.dev/s?module-type=image-object-detection). \n",
    "- You can select a model to see more information about it and copy the URL so you can download it to your workspace. \n",
    "- We selected a [inception resnet version 2](https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1)\n",
    "- You can also modify this following cell to choose the other model that we selected, [ssd mobilenet version 2](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C9pCzz4uy20U"
   },
   "outputs": [],
   "source": [
    "# you can switch the commented lines here to pick the other model\n",
    "\n",
    "# inception resnet version 2\n",
    "module_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"\n",
    "\n",
    "# You can choose ssd mobilenet version 2 instead and compare the results\n",
    "#module_handle = \"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3trj5FbhCR6"
   },
   "source": [
    "#### Load the model\n",
    "\n",
    "Next, you'll load the model specified by the `module_handle`.\n",
    "- This will take a few minutes to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0WHkGDHfhCR6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "model = hub.load(module_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ey0FpHGhCR6"
   },
   "source": [
    "#### Choose the default signature\n",
    "\n",
    "Some models in the Tensorflow hub can be used for different tasks. So each model's documentation should show what *signature* to use when running the model. \n",
    "- If you want to see if a model has more than one signature then you can do something like `print(hub.load(module_handle).signatures.keys())`. In your case, the models you will be using only have the `default` signature so you don't have to worry about other types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "X1BU7AGthCR6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView(_SignatureMap({'default': <ConcreteFunction pruned(images) at 0x7FC0BABD74F0>}))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the available signatures for this particular model\n",
    "model.signatures.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfc9ax9hhCR6"
   },
   "source": [
    "Please choose the 'default' signature for your object detector.\n",
    "- For object detection models, its 'default' signature will accept a batch of image tensors and output a dictionary describing the objects detected, which is what you'll want here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pzwR5zE_hCR7"
   },
   "outputs": [],
   "source": [
    "detector = model.signatures['default']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wvb-3r3thCR7"
   },
   "source": [
    "### download_and_resize_image\n",
    "\n",
    "This function downloads an image specified by a given \"url\", pre-processes it, and then saves it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ucsxak_qhCR7"
   },
   "outputs": [],
   "source": [
    "def download_and_resize_image(url, new_width=256, new_height=256):\n",
    "    '''\n",
    "    Fetches an image online, resizes it and saves it locally.\n",
    "    \n",
    "    Args:\n",
    "        url (string) -- link to the image\n",
    "        new_width (int) -- size in pixels used for resizing the width of the image\n",
    "        new_height (int) -- size in pixels used for resizing the length of the image\n",
    "        \n",
    "    Returns:\n",
    "        (string) -- path to the saved image\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # create a temporary file ending with \".jpg\"\n",
    "    _, filename = tempfile.mkstemp(suffix=\".jpg\")\n",
    "    \n",
    "    # opens the given URL\n",
    "    response = urlopen(url)\n",
    "    \n",
    "    # reads the image fetched from the URL\n",
    "    image_data = response.read()\n",
    "    \n",
    "    # puts the image data in memory buffer\n",
    "    image_data = BytesIO(image_data)\n",
    "    \n",
    "    # opens the image\n",
    "    pil_image = Image.open(image_data)\n",
    "    \n",
    "    # resizes the image. will crop if aspect ratio is different.\n",
    "    pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n",
    "    \n",
    "    # converts to the RGB colorspace\n",
    "    pil_image_rgb = pil_image.convert(\"RGB\")\n",
    "    \n",
    "    # saves the image to the temporary file created earlier\n",
    "    pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n",
    "    \n",
    "    print(\"Image downloaded to %s.\" % filename)\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7qodEJHhCR7"
   },
   "source": [
    "### Download and preprocess an image\n",
    "\n",
    "Now, using `download_and_resize_image` you can get a sample image online and save it locally. \n",
    "- We've provided a URL for you, but feel free to choose another image to run through the object detector.\n",
    "- You can use the original width and height of the image but feel free to modify it and see what results you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xHTDalVrhCR7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8789/3150208785.py:31: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image downloaded to /tmp/tmp3amf64r1.jpg.\n"
     ]
    }
   ],
   "source": [
    "# You can choose a different URL that points to an image of your choice\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/f/fb/20130807_dublin014.JPG\"\n",
    "\n",
    "# download the image and use the original height and width\n",
    "downloaded_image_path = download_and_resize_image(image_url, 3872, 2592)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVNXUKMIhCR7"
   },
   "source": [
    "### run_detector\n",
    "\n",
    "This function will take in the object detection model `detector` and the path to a sample image, then use this model to detect objects and display its predicted class categories and detection boxes.\n",
    "- run_detector uses `load_image` to convert the image into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wkkiQzKlhCR7"
   },
   "outputs": [],
   "source": [
    "def load_img(path):\n",
    "    '''\n",
    "    Loads a JPEG image and converts it to a tensor.\n",
    "    \n",
    "    Args:\n",
    "        path (string) -- path to a locally saved JPEG image\n",
    "    \n",
    "    Returns:\n",
    "        (tensor) -- an image tensor\n",
    "    '''\n",
    "    \n",
    "    # read the file\n",
    "    img = tf.io.read_file(path)\n",
    "    \n",
    "    # convert to a tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def run_detector(detector, path):\n",
    "    '''\n",
    "    Runs inference on a local file using an object detection model.\n",
    "    \n",
    "    Args:\n",
    "        detector (model) -- an object detection model loaded from TF Hub\n",
    "        path (string) -- path to an image saved locally\n",
    "    '''\n",
    "    \n",
    "    # load an image tensor from a local file path\n",
    "    img = load_img(path)\n",
    "\n",
    "    # add a batch dimension in front of the tensor\n",
    "    converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
    "    \n",
    "    # run inference using the model\n",
    "    result = detector(converted_img)\n",
    "\n",
    "    # save the results in a dictionary\n",
    "    result = {key:value.numpy() for key,value in result.items()}\n",
    "\n",
    "    # print results\n",
    "    print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n",
    "\n",
    "    print(result[\"detection_scores\"])\n",
    "    print(result[\"detection_class_entities\"])\n",
    "    print(result[\"detection_boxes\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSEeJSkxhCR7"
   },
   "source": [
    "### Run inference on the image\n",
    "\n",
    "You can run your detector by calling the `run_detector` function. This will print the number of objects found followed by three lists: \n",
    "\n",
    "* The detection scores of each object found (i.e. how confident the model is), \n",
    "* The classes of each object found, \n",
    "* The bounding boxes of each object\n",
    "\n",
    "You will see how to overlay this information on the original image in the next sections and in this week's assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "csanHvDIz4_t"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 23:57:40.154945: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 objects.\n",
      "[0.6422984  0.60824144 0.59652275 0.5937611  0.57701325 0.5769974\n",
      " 0.5768606  0.49604997 0.47756952 0.4736997  0.43105075 0.4138815\n",
      " 0.4077675  0.3999057  0.39596006 0.3803267  0.36254266 0.35481912\n",
      " 0.34390035 0.33957535 0.28821626 0.28000224 0.2547309  0.24532802\n",
      " 0.23076427 0.20000413 0.1782679  0.17733012 0.17339543 0.17200382\n",
      " 0.1657778  0.16451773 0.15528928 0.15454207 0.14807333 0.13604815\n",
      " 0.12518232 0.12455117 0.11767121 0.11580309 0.1145955  0.11169682\n",
      " 0.10969996 0.10543139 0.10173623 0.09649674 0.09526896 0.08696561\n",
      " 0.08613813 0.0818931  0.0804579  0.08020331 0.07396857 0.07252879\n",
      " 0.07184307 0.07154159 0.07078226 0.0705822  0.06969979 0.06787702\n",
      " 0.0614849  0.05930711 0.05914062 0.05813911 0.05801317 0.05776424\n",
      " 0.05679617 0.05379872 0.05271911 0.05194525 0.05076057 0.044547\n",
      " 0.04385839 0.0436633  0.04327364 0.04285271 0.04104624 0.0389266\n",
      " 0.03779633 0.03701331 0.03568832 0.03487361 0.03423513 0.03384284\n",
      " 0.03377624 0.03301959 0.03296954 0.03139564 0.03137915 0.03121648\n",
      " 0.03086422 0.03085947 0.0284633  0.02809238 0.02732026 0.02728349\n",
      " 0.02718221 0.0268145  0.02640964 0.02602408]\n",
      "[b'Person' b'Person' b'Person' b'Person' b'Building' b'Person' b'Footwear'\n",
      " b'Bicycle' b'Window' b'Building' b'Person' b'Bicycle' b'Building'\n",
      " b'Wheel' b'Building' b'Person' b'Building' b'Wheel' b'Window' b'Window'\n",
      " b'Person' b'Van' b'Bicycle wheel' b'Person' b'Window' b'Window' b'Window'\n",
      " b'Person' b'Window' b'Building' b'Man' b'Person' b'Woman' b'Clothing'\n",
      " b'Bicycle wheel' b'Window' b'Person' b'Window' b'Land vehicle' b'Bicycle'\n",
      " b'Land vehicle' b'Window' b'Clothing' b'Land vehicle' b'House' b'House'\n",
      " b'Person' b'Man' b'Window' b'Window' b'Footwear' b'Clothing' b'Man'\n",
      " b'Man' b'House' b'Clothing' b'Window' b'Person' b'Person' b'Building'\n",
      " b'Furniture' b'Jeans' b'Person' b'Land vehicle' b'Man' b'Woman' b'Person'\n",
      " b'Person' b'Window' b'Person' b'Window' b'Man' b'Man' b'Car' b'Person'\n",
      " b'Clothing' b'Car' b'Window' b'House' b'Person' b'Chair' b'Window'\n",
      " b'Clothing' b'Person' b'Clothing' b'Man' b'Tire' b'Window' b'Van'\n",
      " b'Land vehicle' b'Man' b'Window' b'Window' b'Car' b'Trousers' b'Person'\n",
      " b'Bicycle' b'Man' b'Clothing' b'Vehicle']\n",
      "[[5.1262373e-01 5.2931476e-01 6.0180312e-01 5.5221659e-01]\n",
      " [5.1946998e-01 6.0130179e-01 6.4622283e-01 6.3442194e-01]\n",
      " [5.0576425e-01 5.0050801e-01 6.0119104e-01 5.2308583e-01]\n",
      " [4.8614067e-01 4.1263825e-01 6.7864007e-01 4.5991835e-01]\n",
      " [0.0000000e+00 4.8922133e-03 7.7281022e-01 4.0302271e-01]\n",
      " [4.9531698e-01 9.2359799e-01 8.3610147e-01 9.9907708e-01]\n",
      " [8.1415451e-01 9.5595604e-01 8.4242690e-01 9.8637319e-01]\n",
      " [5.7746404e-01 3.6652529e-01 7.1301693e-01 4.8331395e-01]\n",
      " [0.0000000e+00 1.1923673e-01 2.2393158e-01 1.8404545e-01]\n",
      " [7.7654906e-02 4.1286403e-01 5.7902497e-01 5.6055570e-01]\n",
      " [5.1397073e-01 7.4801290e-01 5.9199315e-01 7.6663870e-01]\n",
      " [5.7099390e-01 3.6219662e-01 7.0760262e-01 4.3005288e-01]\n",
      " [1.7108764e-02 6.8510038e-01 5.5543059e-01 8.1064069e-01]\n",
      " [6.3221627e-01 3.6008292e-01 7.0401537e-01 4.1172430e-01]\n",
      " [0.0000000e+00 7.9758710e-01 6.7285186e-01 1.0000000e+00]\n",
      " [5.0032049e-01 3.7766916e-01 6.3359994e-01 4.1415647e-01]\n",
      " [0.0000000e+00 2.1828833e-01 6.5795410e-01 4.3415451e-01]\n",
      " [6.4101005e-01 4.4525203e-01 7.0294243e-01 4.8334819e-01]\n",
      " [2.7514521e-03 9.6662909e-01 1.5331748e-01 9.9994993e-01]\n",
      " [1.9114749e-03 0.0000000e+00 1.3984150e-01 2.6468767e-02]\n",
      " [5.0509632e-01 3.6065596e-01 6.3818991e-01 3.8549680e-01]\n",
      " [4.8310757e-01 6.1952341e-01 5.6262660e-01 6.6156656e-01]\n",
      " [6.3143635e-01 3.6046678e-01 7.0416737e-01 4.1143787e-01]\n",
      " [4.9782380e-01 3.6471906e-01 6.6119683e-01 4.0430561e-01]\n",
      " [2.1914986e-01 3.4892309e-01 3.3834842e-01 3.7730470e-01]\n",
      " [1.2475122e-01 2.5096697e-01 2.7960333e-01 2.8162521e-01]\n",
      " [4.2317811e-02 8.7460649e-01 2.5272831e-01 9.1305840e-01]\n",
      " [5.1332021e-01 6.7933238e-01 5.5109978e-01 6.9256610e-01]\n",
      " [1.5647812e-01 4.4344506e-01 2.2212575e-01 4.7567564e-01]\n",
      " [2.5799122e-01 5.6752908e-01 5.3126621e-01 6.8739468e-01]\n",
      " [5.0184113e-01 9.2158103e-01 8.3678156e-01 1.0000000e+00]\n",
      " [5.2357787e-01 5.7047135e-01 5.8423769e-01 5.9118795e-01]\n",
      " [5.1902717e-01 5.9979767e-01 6.4608830e-01 6.3395023e-01]\n",
      " [5.2391821e-01 9.2504233e-01 8.1090951e-01 9.9802256e-01]\n",
      " [6.3843125e-01 4.4295394e-01 7.0177031e-01 4.8410574e-01]\n",
      " [3.4474768e-02 3.5549229e-01 1.6192172e-01 3.7487409e-01]\n",
      " [4.8836401e-01 4.5370150e-01 6.2296945e-01 4.7984284e-01]\n",
      " [8.8614464e-04 3.0768961e-01 1.0643218e-01 3.3210945e-01]\n",
      " [4.8287660e-01 6.1987525e-01 5.6495380e-01 6.6062629e-01]\n",
      " [6.0928619e-01 4.2691821e-01 7.0520508e-01 4.8701346e-01]\n",
      " [5.8183086e-01 3.6499459e-01 7.1402943e-01 4.8454684e-01]\n",
      " [3.5173228e-01 9.7487772e-01 5.5320495e-01 9.9888134e-01]\n",
      " [5.2372092e-01 7.4927843e-01 5.8529502e-01 7.6536000e-01]\n",
      " [5.6944096e-01 3.5996661e-01 7.0888460e-01 4.2871848e-01]\n",
      " [1.7716039e-02 7.9189031e-06 7.2423309e-01 4.1113979e-01]\n",
      " [0.0000000e+00 7.9736412e-01 6.7840677e-01 9.9998558e-01]\n",
      " [5.2309221e-01 5.8168483e-01 5.8700871e-01 6.0155761e-01]\n",
      " [4.8464936e-01 4.1058400e-01 6.9454116e-01 4.6322626e-01]\n",
      " [8.0978647e-02 3.8451165e-01 2.0744525e-01 4.1174090e-01]\n",
      " [0.0000000e+00 1.2345720e-02 1.4040795e-01 2.4744976e-02]\n",
      " [6.2985927e-01 6.1496639e-01 6.4496815e-01 6.2533790e-01]\n",
      " [5.3646809e-01 6.0176432e-01 6.3454348e-01 6.3437271e-01]\n",
      " [5.1453888e-01 7.4773562e-01 5.9197229e-01 7.6687175e-01]\n",
      " [5.0642818e-01 5.0045890e-01 6.0060155e-01 5.2331811e-01]\n",
      " [0.0000000e+00 2.1054728e-01 6.5165776e-01 4.3445224e-01]\n",
      " [5.0920343e-01 4.1614777e-01 6.6910475e-01 4.5951584e-01]\n",
      " [4.6219253e-03 8.0310804e-01 1.6005209e-01 8.4032220e-01]\n",
      " [5.2601057e-01 5.6857181e-01 5.7968128e-01 5.8291328e-01]\n",
      " [4.8956904e-01 4.5452106e-01 5.7265347e-01 4.7652036e-01]\n",
      " [0.0000000e+00 7.0721316e-01 6.1812693e-01 8.6559439e-01]\n",
      " [5.7426655e-01 2.6724941e-01 6.5759522e-01 3.2035890e-01]\n",
      " [6.7167199e-01 9.4027776e-01 8.2133138e-01 9.8936868e-01]\n",
      " [5.1740557e-01 7.5712180e-01 5.8855182e-01 7.7154803e-01]\n",
      " [6.1258435e-01 4.2750174e-01 7.0603263e-01 4.8819956e-01]\n",
      " [5.0273573e-01 3.7497029e-01 6.4669323e-01 4.1262162e-01]\n",
      " [4.9321544e-01 9.2400664e-01 8.3738810e-01 9.9782264e-01]\n",
      " [4.8628405e-01 4.4456902e-01 6.2579525e-01 4.7332060e-01]\n",
      " [5.2430367e-01 5.6152529e-01 5.7857084e-01 5.8035278e-01]\n",
      " [0.0000000e+00 2.4436575e-01 6.0531300e-02 2.9349202e-01]\n",
      " [5.2345896e-01 5.5779457e-01 5.7944196e-01 5.7347631e-01]\n",
      " [8.3380006e-03 2.4206594e-01 4.9578413e-02 2.8329051e-01]\n",
      " [5.0547194e-01 3.6254400e-01 6.3540047e-01 3.9002120e-01]\n",
      " [5.1342207e-01 6.7938036e-01 5.5071425e-01 6.9248426e-01]\n",
      " [5.2668035e-01 6.2708205e-01 5.6337380e-01 6.5401840e-01]\n",
      " [5.1071990e-01 5.2229112e-01 5.9762067e-01 5.4613191e-01]\n",
      " [5.1305795e-01 5.0094986e-01 5.9555578e-01 5.2252835e-01]\n",
      " [5.1536328e-01 6.2414813e-01 5.6404734e-01 6.5801829e-01]\n",
      " [4.2979082e-01 8.2878554e-01 5.9015876e-01 8.6417776e-01]\n",
      " [8.4440947e-02 4.0708455e-01 5.8386302e-01 5.5852729e-01]\n",
      " [5.0450206e-01 3.8936284e-01 6.1598939e-01 4.1999593e-01]\n",
      " [5.7320464e-01 2.6670381e-01 6.6609180e-01 3.1866449e-01]\n",
      " [2.8825304e-01 5.8369554e-04 4.1442472e-01 3.6543187e-02]\n",
      " [5.1598698e-01 3.8057986e-01 5.9768271e-01 4.1156727e-01]\n",
      " [5.2251548e-01 5.3650224e-01 5.9740996e-01 5.5327350e-01]\n",
      " [4.9734285e-01 4.5540634e-01 5.8417076e-01 4.7798419e-01]\n",
      " [5.0072151e-01 3.6432165e-01 6.5953642e-01 4.0310767e-01]\n",
      " [6.2730837e-01 3.6110201e-01 7.0598483e-01 4.0981957e-01]\n",
      " [1.1576233e-02 3.0822963e-01 9.7194359e-02 3.2491919e-01]\n",
      " [5.1085901e-01 6.2412798e-01 5.6355262e-01 6.5806717e-01]\n",
      " [5.1275134e-01 6.2368506e-01 5.6251687e-01 6.5756243e-01]\n",
      " [5.2091753e-01 6.0124427e-01 6.4555007e-01 6.3486522e-01]\n",
      " [0.0000000e+00 9.9825896e-03 1.3646212e-01 3.1599715e-02]\n",
      " [4.0110663e-01 8.8498741e-01 5.8034980e-01 9.3902177e-01]\n",
      " [4.8625600e-01 6.2034112e-01 5.6362754e-01 6.6030347e-01]\n",
      " [6.6411579e-01 9.3983281e-01 8.1753016e-01 9.8859632e-01]\n",
      " [5.1439118e-01 6.6999948e-01 5.5103672e-01 6.8414575e-01]\n",
      " [5.6296080e-01 3.8115385e-01 6.7691207e-01 4.1963542e-01]\n",
      " [5.1363021e-01 5.2953970e-01 6.0218978e-01 5.5250055e-01]\n",
      " [5.3821856e-01 9.2813838e-01 7.1231830e-01 9.9952537e-01]\n",
      " [4.8112500e-01 6.1799109e-01 5.6419271e-01 6.6188407e-01]]\n"
     ]
    }
   ],
   "source": [
    "# runs the object detection model and prints information about the objects found\n",
    "run_detector(detector, downloaded_image_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
